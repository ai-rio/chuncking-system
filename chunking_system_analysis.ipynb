{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Notebook: `chunking_system_analysis.ipynb`\n",
    "\n",
    "### This notebook will provide an interactive interface to test and analyze the document chunking system.\n",
    "\n",
    "### **Cell 0: Install Requirements**\n",
    "\n",
    "### - This cell will ensure all necessary Python packages are installed from `requirements.txt`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 0: Install Requirements\n",
    "# Ensure you are in the project's root directory or adjust path accordingly\n",
    "# This command assumes requirements.txt is in the notebook's parent directory\n",
    "# or the project root if the notebook is run from there.\n",
    "!pip install -r requirements.txt --index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "# If 'uv' is preferred and available in your environment, you could use:\n",
    "# !uv pip install -r requirements.txt --index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "# This ensures the kernel has access to all installed packages immediately.\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Define project root for consistent pathing\n",
    "# This tries to find the project root from common Jupyter contexts\n",
    "current_dir = os.getcwd()\n",
    "if os.path.basename(current_dir) == 'src': # If notebook is in src/\n",
    "    project_root = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "elif os.path.basename(os.path.basename(current_dir)) == 'chunking_system_analysis.ipynb': # If notebook in data/input or similar\n",
    "    project_root = os.path.abspath(os.path.join(current_dir, '..', '..'))\n",
    "else: # Assume notebook is in project root\n",
    "    project_root = current_dir\n",
    "\n",
    "# Add the project root and src directory to sys.path\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "if os.path.join(project_root, 'src') not in sys.path:\n",
    "    sys.path.insert(0, os.path.join(project_root, 'src'))\n",
    "\n",
    "print(\"Requirements installation attempted.\")\n",
    "print(f\"Project root set to: {project_root}\")\n",
    "print(f\"sys.path updated: {sys.path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 1: Setup and Imports** (Original Cell 1, now shifted)\n",
    "\n",
    "## - Import necessary modules from our system (`HybridMarkdownChunker`, `ChunkQualityEvaluator`, `MetadataEnricher`, `config`, `FileHandler`).\n",
    "\n",
    "- Set up paths to input/output directories.\n",
    "\n",
    "- Display a reminder about `.env` file for API keys.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup and Imports\n",
    "import os\n",
    "import sys\n",
    "import asyncio\n",
    "from IPython.display import display, Markdown, JSON # For richer display in Jupyter\n",
    "\n",
    "# Add the parent directory of 'src' to the Python path\n",
    "# This allows importing modules from src.chunkers, src.utils, etc.\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Check if we are in the 'document_chunking_system' root or a subdirectory\n",
    "if os.path.basename(os.getcwd()) == 'document_chunking_system':\n",
    "    base_dir = os.getcwd()\n",
    "elif os.path.basename(os.getcwd()) == 'src': # If running from src/ or src/chunkers\n",
    "    base_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "else: # Assume we are in the project root if it's not explicitly handled\n",
    "    base_dir = os.getcwd()\n",
    "\n",
    "# Adjust sys.path to ensure src modules are discoverable\n",
    "if os.path.join(base_dir, 'src') not in sys.path:\n",
    "    sys.path.insert(0, os.path.join(base_dir, 'src'))\n",
    "\n",
    "# Now import our modules\n",
    "from chunkers.hybrid_chunker import HybridMarkdownChunker\n",
    "from chunkers.evaluators import ChunkQualityEvaluator\n",
    "from utils.file_handler import FileHandler\n",
    "from utils.metadata_enricher import MetadataEnricher\n",
    "from config.settings import config\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Define file paths (relative to project root for consistency)\n",
    "config.INPUT_DIR = os.path.join(base_dir, \"data\", \"input\", \"markdown_files\")\n",
    "config.OUTPUT_DIR = os.path.join(base_dir, \"data\", \"output\")\n",
    "config.TEMP_DIR = os.path.join(base_dir, \"data\", \"temp\")\n",
    "\n",
    "os.makedirs(os.path.join(config.OUTPUT_DIR, \"chunks\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(config.OUTPUT_DIR, \"reports\"), exist_ok=True)\n",
    "\n",
    "print(\"System modules and configurations loaded.\")\n",
    "print(f\"Project base directory set to: {base_dir}\")\n",
    "print(f\"LLM Metadata Enrichment Enabled: {config.ENABLE_LLM_METADATA_ENRICHMENT}\")\n",
    "print(f\"LLM Image Description Enabled: {config.ENABLE_LLM_IMAGE_DESCRIPTION}\")\n",
    "if not config.GEMINI_API_KEY:\n",
    "    display(Markdown(\"--- \\n**WARNING: `GEMINI_API_KEY` is not set in your `.env` file.** \\nLLM-based summaries and image descriptions will use mock data. \\nTo enable live LLM calls, please set `GEMINI_API_KEY=\\\"YOUR_KEY_HERE\\\"` in your `.env` file. \\n---\"))\n",
    "\n",
    "# Initialize core components\n",
    "chunker = HybridMarkdownChunker(enable_semantic=True) # Semantic chunking enabled by default for testing\n",
    "evaluator = ChunkQualityEvaluator()\n",
    "metadata_enricher = MetadataEnricher()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 2: Helper Functions for Notebook Display**\n",
    "\n",
    "## - Functions to easily display chunks and reports.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Helper Functions for Notebook Display\n",
    "def display_chunks(chunks: List[Document], title=\"Generated Chunks\"):\n",
    "    display(Markdown(f\"### {title} ({len(chunks)} Chunks)\"))\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        display(Markdown(f\"#### Chunk {i} (Index: {chunk.metadata.get('chunk_index')})\"))\n",
    "        display(Markdown(f\"**Content Type:** `{chunk.metadata.get('content_type', 'text')}`\"))\n",
    "        display(Markdown(f\"**Chunk Strategy:** `{chunk.metadata.get('chunking_strategy', 'N/A')}`\"))\n",
    "        display(Markdown(f\"**Tokens:** {chunk.metadata.get('chunk_tokens')}, **Words:** {chunk.metadata.get('word_count')}\"))\n",
    "        if 'llm_summary' in chunk.metadata:\n",
    "            display(Markdown(f\"**LLM Summary:** {chunk.metadata['llm_summary']}\"))\n",
    "        if 'image_alt_text' in chunk.metadata:\n",
    "            display(Markdown(f\"**Original Image Alt Text:** {chunk.metadata['image_alt_text']}\"))\n",
    "        if 'image_url' in chunk.metadata:\n",
    "            display(Markdown(f\"**Original Image URL:** {chunk.metadata['image_url']}\"))\n",
    "        \n",
    "        display(Markdown(\"```markdown\\n\" + chunk.page_content + \"\\n```\"))\n",
    "        display(Markdown(\"---\"))\n",
    "\n",
    "def display_report(report_content: str):\n",
    "    display(Markdown(\"### Quality Evaluation Report\"))\n",
    "    display(Markdown(report_content))\n",
    "\n",
    "async def process_and_display(file_path: str, chunker_instance: HybridMarkdownChunker, evaluator_instance: ChunkQualityEvaluator, enricher_instance: MetadataEnricher):\n",
    "    \"\"\"Processes a file, enriches, evaluates, and displays results.\"\"\"\n",
    "    print(f\"\\n--- Processing Document: {os.path.basename(file_path)} ---\")\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    initial_metadata = {\n",
    "        'source_file': os.path.basename(file_path),\n",
    "        'document_type': 'markdown'\n",
    "    }\n",
    "\n",
    "    chunks = await chunker_instance.chunk_document(content, initial_metadata)\n",
    "    print(f\"Initial chunking generated {len(chunks)} chunks.\")\n",
    "\n",
    "    enriched_chunks = await enricher_instance.enrich_chunks_with_llm_summaries(chunks)\n",
    "    print(f\"Enrichment completed for {len(enriched_chunks)} chunks.\")\n",
    "\n",
    "    display_chunks(enriched_chunks, f\"Chunks from {os.path.basename(file_path)}\")\n",
    "\n",
    "    report_content = evaluator_instance.generate_report(enriched_chunks)\n",
    "    display_report(report_content)\n",
    "\n",
    "    return enriched_chunks # Return chunks for further inspection if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 3: Chunking Example - Prose Document (`sample_prose_document.md`)**\n",
    "\n",
    "### - Run chunking and evaluation for a prose document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Chunking Example - Prose Document\n",
    "PROSE_FILE = os.path.join(config.INPUT_DIR, \"sample_prose_document.md\")\n",
    "\n",
    "# Ensure dummy file exists for demonstration\n",
    "if not os.path.exists(PROSE_FILE):\n",
    "    print(f\"Creating dummy prose test file: {PROSE_FILE}\")\n",
    "    dummy_content_prose = \"\"\"\n",
    "# The Future of Artificial Intelligence\n",
    "\n",
    "Artificial intelligence (AI) is rapidly transforming industries and daily life. From self-driving cars to advanced medical diagnostics, AI's capabilities are expanding at an unprecedented pace. This technology promises to solve complex problems, enhance efficiency, and unlock new frontiers of innovation. However, its development also presents ethical and societal challenges that require careful consideration.\n",
    "\n",
    "## Machine Learning and Deep Learning\n",
    "\n",
    "At the core of many AI advancements are machine learning (ML) and deep learning (DL). Machine learning algorithms enable systems to learn from data without explicit programming. Deep learning, a subset of ML, uses neural networks with multiple layers to learn complex patterns. These techniques are behind breakthroughs in image recognition, natural language processing, and predictive analytics. The sheer volume of data available today fuels these algorithms, allowing them to achieve remarkable accuracy.\n",
    "\n",
    "### The Role of Large Language Models (LLMs)\n",
    "\n",
    "Large Language Models (LLMs) like Gemini, GPT, and Llama have revolutionized natural language understanding and generation. These models are trained on vast datasets of text and code, allowing them to perform tasks such as translation, summarization, and creative writing. They represent a significant leap towards more human-like AI interactions. The ability of LLMs to generate coherent and contextually relevant text has opened new possibilities for applications in customer service, content creation, and education.\n",
    "\n",
    "## Ethical Considerations and Societal Impact\n",
    "\n",
    "The increasing power of AI also brings forth critical ethical considerations. Issues such as algorithmic bias, privacy concerns, job displacement, and the potential for misuse of AI technologies are paramount. Ensuring fairness, transparency, and accountability in AI systems is essential for their responsible deployment. Discussions around AI governance and regulation are gaining momentum as societies grapple with the profound impact of these technologies.\n",
    "\n",
    "### AI in Healthcare\n",
    "\n",
    "AI is poised to transform healthcare by assisting with drug discovery, personalized treatment plans, and early disease detection. AI-powered tools can analyze vast amounts of patient data to identify patterns that human doctors might miss. This can lead to more accurate diagnoses and more effective interventions, ultimately improving patient outcomes. The integration of AI into healthcare systems requires robust validation and careful integration to ensure patient safety and data security.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The journey of AI is far from over. As researchers continue to push the boundaries of what's possible, AI will undoubtedly become even more integrated into the fabric of society. Navigating its complexities, harnessing its potential, and mitigating its risks will be a collective endeavor, requiring collaboration across technology, policy, and ethics. The future is exciting, but it also demands thoughtful stewardship of this powerful technology.\n",
    "    \"\"\"\n",
    "    with open(PROSE_FILE, 'w', encoding='utf-8') as f:\n",
    "        f.write(dummy_content_prose)\n",
    "\n",
    "prose_chunks = await process_and_display(PROSE_FILE, chunker, evaluator, metadata_enricher)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 4: Chunking Example - Table Document (`sample_table_document.md`)**\n",
    "\n",
    "### - Run chunking and evaluation for a document containing tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Chunking Example - Table Document\n",
    "TABLE_FILE = os.path.join(config.INPUT_DIR, \"sample_table_document.md\")\n",
    "\n",
    "# Ensure dummy file exists for demonstration\n",
    "if not os.path.exists(TABLE_FILE):\n",
    "    print(f\"Creating dummy table test file: {TABLE_FILE}\")\n",
    "    dummy_content_table = \"\"\"\n",
    "# Document with Tables\n",
    "\n",
    "This is some introductory text before a table.\n",
    "\n",
    "| Header 1 | Header 2 | Header 3 |\n",
    "|----------|----------|----------|\n",
    "| Row 1 Col 1 | Row 1 Col 2 | Row 1 Col 3 |\n",
    "| Row 2 Col 1 | Row 2 Col 2 | Row 2 Col 3 |\n",
    "| This is a very long piece of text that spans multiple words in a single cell, to test how the chunker handles long content within a table. It should ideally split this row if it exceeds the token limit set for table chunks. | Another cell | Last cell of a long row |\n",
    "| Row 4 Col 1 | Row 4 Col 2 | Row 4 Col 3 |\n",
    "\n",
    "Some text after the first table.\n",
    "\n",
    "## Another Section with a Small Table\n",
    "\n",
    "| Key | Value |\n",
    "|-----|-------|\n",
    "| Apple | Fruit |\n",
    "| Carrot| Veggie|\n",
    "\n",
    "End of document.\n",
    "    \"\"\"\n",
    "    with open(TABLE_FILE, 'w', encoding='utf-8') as f:\n",
    "        f.write(dummy_content_table)\n",
    "\n",
    "table_chunks = await process_and_display(TABLE_FILE, chunker, evaluator, metadata_enricher)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 5: Chunking Example - Image Document (`sample_image_document.md`)**\n",
    "\n",
    "### - Run chunking and evaluation for a document containing images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Chunking Example - Image Document\n",
    "IMAGE_FILE = os.path.join(config.INPUT_DIR, \"sample_image_document.md\")\n",
    "\n",
    "# Ensure dummy file exists for demonstration\n",
    "if not os.path.exists(IMAGE_FILE):\n",
    "    print(f\"Creating dummy image test file: {IMAGE_FILE}\")\n",
    "    dummy_content_image = \"\"\"\n",
    "# Document with Images\n",
    "\n",
    "This document contains text and some images that need to be processed.\n",
    "\n",
    "## Section 1: Introduction to AI\n",
    "\n",
    "Artificial intelligence (AI) is transforming many aspects of our lives. From smart assistants to complex data analysis, AI is becoming increasingly prevalent.\n",
    "\n",
    "Here's an example of an AI model's architecture:\n",
    "![Neural Network Architecture](https://placehold.co/600x400/FF0000/FFFFFF?text=Neural%20Network)\n",
    "A visual representation of a deep neural network, showing layers of interconnected nodes.\n",
    "\n",
    "## Section 2: Data Visualization\n",
    "\n",
    "Data visualization is key to understanding complex datasets. Charts and graphs help us interpret trends and patterns.\n",
    "\n",
    "This chart illustrates market trends over the last quarter:\n",
    "![Market Trends Chart](https://placehold.co/800x500/00FF00/000000?text=Market%20Trends%20Q1)\n",
    "A bar chart depicting sales performance across different product categories for the first quarter.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Images play a crucial role in conveying information effectively.\n",
    "    \"\"\"\n",
    "    with open(IMAGE_FILE, 'w', encoding='utf-8') as f:\n",
    "        f.write(dummy_content_image)\n",
    "\n",
    "# For image processing, ensure ENABLE_LLM_IMAGE_DESCRIPTION is True in settings.py\n",
    "# and GEMINI_API_KEY is set in .env for live LLM calls.\n",
    "image_chunks = await process_and_display(IMAGE_FILE, chunker, evaluator, metadata_enricher)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 6: Interactive Experimentation / Custom Runs**\n",
    "\n",
    "### - Instructions on how to modify parameters and run custom tests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Cell 6: Interactive Experimentation / Custom Runs\n",
    "display(Markdown(\"### Interactive Experimentation / Custom Runs\"))\n",
    "display(Markdown(\"\"\"\n",
    "This section demonstrates how to create new `HybridMarkdownChunker` instances with different parameters\n",
    "and process documents to see the effects.\n",
    "\n",
    "**Important Note:** If you modify values in `src/config/settings.py` (e.g., `DEFAULT_CHUNK_SIZE`, `SEMANTIC_SIMILARITY_THRESHOLD`, `ENABLE_LLM_IMAGE_DESCRIPTION`), you **MUST restart your Jupyter Kernel** (Kernel -> Restart) and then re-run **Cell 1** to ensure the changes are loaded.\n",
    "\"\"\"))\n",
    "\n",
    "# --- Experiment 1: Change Chunk Size for Prose Document ---\n",
    "display(Markdown(\"#### Experiment 1: Prose Document with Smaller Chunk Size\"))\n",
    "\n",
    "# Create a new chunker instance with a smaller chunk size for more splits\n",
    "# This will use the new chunk_size directly, overriding the config.DEFAULT_CHUNK_SIZE for this instance\n",
    "chunker_small_prose = HybridMarkdownChunker(chunk_size=200, enable_semantic=True) \n",
    "\n",
    "# Process the prose document with the new chunker\n",
    "prose_chunks_small_size = await process_and_display(PROSE_FILE, chunker_small_prose, evaluator, metadata_enricher)\n",
    "\n",
    "display(Markdown(\"---\"))\n",
    "\n",
    "# --- Experiment 2: Table Document with different TABLE_CHUNK_MAX_TOKENS ---\n",
    "display(Markdown(\"#### Experiment 2: Table Document with Very Small Table Chunk Limit\"))\n",
    "\n",
    "# Create a new chunker instance with a very small table chunk limit\n",
    "# This will force tables to split into very granular row-by-row chunks\n",
    "chunker_tiny_tables = HybridMarkdownChunker(chunk_size=800, enable_semantic=True) \n",
    "# Note: For this to work, you'd typically modify `TABLE_CHUNK_MAX_TOKENS` in settings.py,\n",
    "# restart kernel, and then rerun cell 1. This is just for demonstration.\n",
    "# For immediate effect, you would need to either:\n",
    "# 1. Directly override it in settings.py and restart.\n",
    "# 2. Modify the chunker's internal config directly (less clean):\n",
    "#    chunker_tiny_tables.config.TABLE_CHUNK_MAX_TOKENS = 20 # This would apply to all subsequent calls\n",
    "\n",
    "# Let's directly process with the default chunker but explain the settings change\n",
    "display(Markdown(f\"\"\"\n",
    "*(To truly see extreme table splitting, you would set `config.TABLE_CHUNK_MAX_TOKENS = 20` in `src/config/settings.py`, restart the kernel, and rerun Cell 1 and this cell.)*\n",
    "\"\"\"))\n",
    "table_chunks_default_settings = await process_and_display(TABLE_FILE, chunker, evaluator, metadata_enricher)\n",
    "\n",
    "display(Markdown(\"---\"))\n",
    "\n",
    "# --- Experiment 3: Disable Semantic Chunking for Prose Document ---\n",
    "display(Markdown(\"#### Experiment 3: Prose Document with Semantic Chunking Disabled\"))\n",
    "\n",
    "# Create a new chunker instance with semantic chunking disabled\n",
    "chunker_no_semantic = HybridMarkdownChunker(enable_semantic=False)\n",
    "\n",
    "# Process the prose document with semantic chunking disabled\n",
    "prose_chunks_no_semantic = await process_and_display(PROSE_FILE, chunker_no_semantic, evaluator, metadata_enricher)\n",
    "\n",
    "display(Markdown(\"---\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
